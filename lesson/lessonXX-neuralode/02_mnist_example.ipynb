{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0229965-cf8f-4066-840f-d83dfc6d2f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macbookair/program/python/neural_networks_course/lesson/lessonXX-neuralode/02_mnist_example.py\n",
      "# FROM https://github.com/rtqichen/torchdiffeq/blob/master/examples/odenet_mnist.py\n",
      "\n",
      "import os\n",
      "import argparse\n",
      "import logging\n",
      "import time\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import DataLoader\n",
      "import torchvision.datasets as datasets\n",
      "import torchvision.transforms as transforms\n",
      "\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n",
      "parser.add_argument('--tol', type=float, default=1e-3)\n",
      "parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n",
      "parser.add_argument('--nepochs', type=int, default=160)\n",
      "parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--lr', type=float, default=0.1)\n",
      "parser.add_argument('--batch_size', type=int, default=128)\n",
      "parser.add_argument('--test_batch_size', type=int, default=1000)\n",
      "\n",
      "parser.add_argument('--save', type=str, default='./experiment1')\n",
      "parser.add_argument('--debug', action='store_true')\n",
      "parser.add_argument('--gpu', type=int, default=0)\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.adjoint:\n",
      "    from torchdiffeq import odeint_adjoint as odeint\n",
      "else:\n",
      "    from torchdiffeq import odeint\n",
      "\n",
      "\n",
      "def conv3x3(in_planes, out_planes, stride=1):\n",
      "    \"\"\"3x3 convolution with padding\"\"\"\n",
      "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
      "\n",
      "\n",
      "def conv1x1(in_planes, out_planes, stride=1):\n",
      "    \"\"\"1x1 convolution\"\"\"\n",
      "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
      "\n",
      "\n",
      "def norm(dim):\n",
      "    return nn.GroupNorm(min(32, dim), dim)\n",
      "\n",
      "\n",
      "class ResBlock(nn.Module):\n",
      "    expansion = 1\n",
      "\n",
      "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
      "        super(ResBlock, self).__init__()\n",
      "        self.norm1 = norm(inplanes)\n",
      "        self.relu = nn.ReLU(inplace=True)\n",
      "        self.downsample = downsample\n",
      "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
      "        self.norm2 = norm(planes)\n",
      "        self.conv2 = conv3x3(planes, planes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        shortcut = x\n",
      "\n",
      "        out = self.relu(self.norm1(x))\n",
      "\n",
      "        if self.downsample is not None:\n",
      "            shortcut = self.downsample(out)\n",
      "\n",
      "        out = self.conv1(out)\n",
      "        out = self.norm2(out)\n",
      "        out = self.relu(out)\n",
      "        out = self.conv2(out)\n",
      "\n",
      "        return out + shortcut\n",
      "\n",
      "\n",
      "class ConcatConv2d(nn.Module):\n",
      "\n",
      "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n",
      "        super(ConcatConv2d, self).__init__()\n",
      "        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n",
      "        self._layer = module(\n",
      "            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n",
      "            bias=bias\n",
      "        )\n",
      "\n",
      "    def forward(self, t, x):\n",
      "        tt = torch.ones_like(x[:, :1, :, :]) * t\n",
      "        ttx = torch.cat([tt, x], 1)\n",
      "        return self._layer(ttx)\n",
      "\n",
      "\n",
      "class ODEfunc(nn.Module):\n",
      "\n",
      "    def __init__(self, dim):\n",
      "        super(ODEfunc, self).__init__()\n",
      "        self.norm1 = norm(dim)\n",
      "        self.relu = nn.ReLU(inplace=True)\n",
      "        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
      "        self.norm2 = norm(dim)\n",
      "        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
      "        self.norm3 = norm(dim)\n",
      "        self.nfe = 0\n",
      "\n",
      "    def forward(self, t, x):\n",
      "        self.nfe += 1\n",
      "        out = self.norm1(x)\n",
      "        out = self.relu(out)\n",
      "        out = self.conv1(t, out)\n",
      "        out = self.norm2(out)\n",
      "        out = self.relu(out)\n",
      "        out = self.conv2(t, out)\n",
      "        out = self.norm3(out)\n",
      "        return out\n",
      "\n",
      "\n",
      "class ODEBlock(nn.Module):\n",
      "\n",
      "    def __init__(self, odefunc):\n",
      "        super(ODEBlock, self).__init__()\n",
      "        self.odefunc = odefunc\n",
      "        self.integration_time = torch.tensor([0, 1]).float()\n",
      "\n",
      "    def forward(self, x):\n",
      "        self.integration_time = self.integration_time.type_as(x)\n",
      "        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n",
      "        return out[1]\n",
      "\n",
      "    @property\n",
      "    def nfe(self):\n",
      "        return self.odefunc.nfe\n",
      "\n",
      "    @nfe.setter\n",
      "    def nfe(self, value):\n",
      "        self.odefunc.nfe = value\n",
      "\n",
      "\n",
      "class Flatten(nn.Module):\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Flatten, self).__init__()\n",
      "\n",
      "    def forward(self, x):\n",
      "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
      "        return x.view(-1, shape)\n",
      "\n",
      "\n",
      "class RunningAverageMeter(object):\n",
      "    \"\"\"Computes and stores the average and current value\"\"\"\n",
      "\n",
      "    def __init__(self, momentum=0.99):\n",
      "        self.momentum = momentum\n",
      "        self.reset()\n",
      "\n",
      "    def reset(self):\n",
      "        self.val = None\n",
      "        self.avg = 0\n",
      "\n",
      "    def update(self, val):\n",
      "        if self.val is None:\n",
      "            self.avg = val\n",
      "        else:\n",
      "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
      "        self.val = val\n",
      "\n",
      "\n",
      "def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n",
      "    if data_aug:\n",
      "        transform_train = transforms.Compose([\n",
      "            transforms.RandomCrop(28, padding=4),\n",
      "            transforms.ToTensor(),\n",
      "        ])\n",
      "    else:\n",
      "        transform_train = transforms.Compose([\n",
      "            transforms.ToTensor(),\n",
      "        ])\n",
      "\n",
      "    transform_test = transforms.Compose([\n",
      "        transforms.ToTensor(),\n",
      "    ])\n",
      "\n",
      "    train_loader = DataLoader(\n",
      "        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n",
      "        shuffle=True, num_workers=2, drop_last=True\n",
      "    )\n",
      "\n",
      "    train_eval_loader = DataLoader(\n",
      "        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n",
      "        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n",
      "    )\n",
      "\n",
      "    test_loader = DataLoader(\n",
      "        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n",
      "        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n",
      "    )\n",
      "\n",
      "    return train_loader, test_loader, train_eval_loader\n",
      "\n",
      "\n",
      "def inf_generator(iterable):\n",
      "    \"\"\"Allows training with DataLoaders in a single infinite loop:\n",
      "        for i, (x, y) in enumerate(inf_generator(train_loader)):\n",
      "    \"\"\"\n",
      "    iterator = iterable.__iter__()\n",
      "    while True:\n",
      "        try:\n",
      "            yield iterator.__next__()\n",
      "        except StopIteration:\n",
      "            iterator = iterable.__iter__()\n",
      "\n",
      "\n",
      "def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n",
      "    initial_learning_rate = args.lr * batch_size / batch_denom\n",
      "\n",
      "    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n",
      "    vals = [initial_learning_rate * decay for decay in decay_rates]\n",
      "\n",
      "    def learning_rate_fn(itr):\n",
      "        lt = [itr < b for b in boundaries] + [True]\n",
      "        i = np.argmax(lt)\n",
      "        return vals[i]\n",
      "\n",
      "    return learning_rate_fn\n",
      "\n",
      "\n",
      "def one_hot(x, K):\n",
      "    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n",
      "\n",
      "\n",
      "def accuracy(model, dataset_loader):\n",
      "    total_correct = 0\n",
      "    for x, y in dataset_loader:\n",
      "        x = x.to(device)\n",
      "        y = one_hot(np.array(y.numpy()), 10)\n",
      "\n",
      "        target_class = np.argmax(y, axis=1)\n",
      "        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n",
      "        total_correct += np.sum(predicted_class == target_class)\n",
      "    return total_correct / len(dataset_loader.dataset)\n",
      "\n",
      "\n",
      "def count_parameters(model):\n",
      "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
      "\n",
      "\n",
      "def makedirs(dirname):\n",
      "    if not os.path.exists(dirname):\n",
      "        os.makedirs(dirname)\n",
      "\n",
      "\n",
      "def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n",
      "    logger = logging.getLogger()\n",
      "    if debug:\n",
      "        level = logging.DEBUG\n",
      "    else:\n",
      "        level = logging.INFO\n",
      "    logger.setLevel(level)\n",
      "    if saving:\n",
      "        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n",
      "        info_file_handler.setLevel(level)\n",
      "        logger.addHandler(info_file_handler)\n",
      "    if displaying:\n",
      "        console_handler = logging.StreamHandler()\n",
      "        console_handler.setLevel(level)\n",
      "        logger.addHandler(console_handler)\n",
      "    logger.info(filepath)\n",
      "    with open(filepath, \"r\") as f:\n",
      "        logger.info(f.read())\n",
      "\n",
      "    for f in package_files:\n",
      "        logger.info(f)\n",
      "        with open(f, \"r\") as package_f:\n",
      "            logger.info(package_f.read())\n",
      "\n",
      "    return logger\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "\n",
      "    makedirs(args.save)\n",
      "    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n",
      "    logger.info(args)\n",
      "\n",
      "    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "    is_odenet = args.network == 'odenet'\n",
      "\n",
      "    if args.downsampling_method == 'conv':\n",
      "        downsampling_layers = [\n",
      "            nn.Conv2d(1, 64, 3, 1),\n",
      "            norm(64),\n",
      "            nn.ReLU(inplace=True),\n",
      "            nn.Conv2d(64, 64, 4, 2, 1),\n",
      "            norm(64),\n",
      "            nn.ReLU(inplace=True),\n",
      "            nn.Conv2d(64, 64, 4, 2, 1),\n",
      "        ]\n",
      "    elif args.downsampling_method == 'res':\n",
      "        downsampling_layers = [\n",
      "            nn.Conv2d(1, 64, 3, 1),\n",
      "            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
      "            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
      "        ]\n",
      "\n",
      "    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n",
      "    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n",
      "\n",
      "    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n",
      "\n",
      "    criterion = nn.CrossEntropyLoss().to(device)\n",
      "\n",
      "    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n",
      "        args.data_aug, args.batch_size, args.test_batch_size\n",
      "    )\n",
      "\n",
      "    data_gen = inf_generator(train_loader)\n",
      "    batches_per_epoch = len(train_loader)\n",
      "\n",
      "    lr_fn = learning_rate_with_decay(\n",
      "        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n",
      "        decay_rates=[1, 0.1, 0.01, 0.001]\n",
      "    )\n",
      "\n",
      "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n",
      "\n",
      "    best_acc = 0\n",
      "    batch_time_meter = RunningAverageMeter()\n",
      "    f_nfe_meter = RunningAverageMeter()\n",
      "    b_nfe_meter = RunningAverageMeter()\n",
      "    end = time.time()\n",
      "\n",
      "    for itr in range(args.nepochs * batches_per_epoch):\n",
      "\n",
      "        for param_group in optimizer.param_groups:\n",
      "            param_group['lr'] = lr_fn(itr)\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "        x, y = data_gen.__next__()\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        logits = model(x)\n",
      "        loss = criterion(logits, y)\n",
      "\n",
      "        if is_odenet:\n",
      "            nfe_forward = feature_layers[0].nfe\n",
      "            feature_layers[0].nfe = 0\n",
      "\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if is_odenet:\n",
      "            nfe_backward = feature_layers[0].nfe\n",
      "            feature_layers[0].nfe = 0\n",
      "\n",
      "        batch_time_meter.update(time.time() - end)\n",
      "        if is_odenet:\n",
      "            f_nfe_meter.update(nfe_forward)\n",
      "            b_nfe_meter.update(nfe_backward)\n",
      "        end = time.time()\n",
      "\n",
      "        if itr % batches_per_epoch == 0:\n",
      "            with torch.no_grad():\n",
      "                train_acc = accuracy(model, train_eval_loader)\n",
      "                val_acc = accuracy(model, test_loader)\n",
      "                if val_acc > best_acc:\n",
      "                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n",
      "                    best_acc = val_acc\n",
      "                logger.info(\n",
      "                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n",
      "                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n",
      "                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n",
      "                        b_nfe_meter.avg, train_acc, val_acc\n",
      "                    )\n",
      "                )\n",
      "Namespace(network='odenet', tol=0.001, adjoint=True, downsampling_method='conv', nepochs=160, data_aug=True, lr=0.1, batch_size=128, test_batch_size=1000, save='./experiment1', debug=False, gpu=0)\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (7): ODEBlock(\n",
      "    (odefunc): ODEfunc(\n",
      "      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv1): ConcatConv2d(\n",
      "        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      (conv2): ConcatConv2d(\n",
      "        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (9): ReLU(inplace=True)\n",
      "  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (11): Flatten()\n",
      "  (12): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Number of parameters: 208266\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "!python 02_mnist_example.py --network odenet --adjoint True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0767237-db9f-472c-a99b-4b4ccb157581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdiffeq\n",
      "  Using cached torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
      "Requirement already satisfied: torch>=1.5.0 in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from torchdiffeq) (2.7.0)\n",
      "Collecting scipy>=1.4.0 (from torchdiffeq)\n",
      "  Downloading scipy-1.15.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from scipy>=1.4.0->torchdiffeq) (2.2.6)\n",
      "Requirement already satisfied: filelock in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from torch>=1.5.0->torchdiffeq) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from torch>=1.5.0->torchdiffeq) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from torch>=1.5.0->torchdiffeq) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from torch>=1.5.0->torchdiffeq) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from torch>=1.5.0->torchdiffeq) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from torch>=1.5.0->torchdiffeq) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/macbookair/program/python/neural_networks_course/env/lib/python3.13/site-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.2)\n",
      "Using cached torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
      "Downloading scipy-1.15.3-cp313-cp313-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy, torchdiffeq\n",
      "Successfully installed scipy-1.15.3 torchdiffeq-0.2.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae8e91-c37c-4a73-9865-d18f5676b23e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
